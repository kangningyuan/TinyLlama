{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qycf3ikvkUqP"
   },
   "source": [
    "## Assignment 1: Build a Toy Llama-2 Language Model\n",
    "\n",
    "> CISC7021 Applied Natural Language Processing (2024/2025)\n",
    "\n",
    "In this assignment, we will prepare a toy language model that employs the **Llama-2** architecture and evaluate the perplexity of the data set.\n",
    "\n",
    "We will learn how to perform continual pre-training of a base language model using the PyTorch and Hugging Face libraries. Detailed instructions for building this language model can be found in the attached notebook file.\n",
    "\n",
    "Acknowledgement: The base model checkpoint is converted from [llama2.c](https://github.com/karpathy/llama2.c) project. The data instances were sampled from [TinyStories](https://huggingface.co/datasets/roneneldan/TinyStories) dataset.\n",
    "\n",
    "---\n",
    "\n",
    "🚨 Please note that running this on CPU may be slow. If running on Google Colab or Kaggle, you can avoid this by going to **Runtime > Change runtime type > Hardware accelerator > GPU > GPU type > T4**. This should be included within the free tier of Colab.\n",
    "\n",
    "---\n",
    "\n",
    "We start by doing a `pip install` of all required libraries.\n",
    "- 🤗 `transformers`, `datasets`, `accelerate` are Huggingface libraries.\n",
    "- By default, Colab has `transformers`, `pytorch` libraries installed. If you are using a local machine, please install them via `pip` or `conda`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UOhVvTEaa_b0"
   },
   "outputs": [],
   "source": [
    "#!pip install torch torchvision torchaudio\n",
    "#!pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mNC4vO-JkUqQ"
   },
   "outputs": [],
   "source": [
    "!pip install datasets accelerate -q"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2WzqztoXkUqQ"
   },
   "source": [
    "### (Optional) Uploading the model/data to Google Colab or Kaggle.\n",
    "\n",
    "Please upload your dataset and model to computational platforms if you are using Colab or Kaggle environments.\n",
    "\n",
    "For Colab users, you can mount your Google Drive files by running the following code snippet:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "kRSpL18W_Zfa",
    "outputId": "c489035e-c816-4552-cea9-133bc0492bf5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/drive\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dqvZRLZYkUqR"
   },
   "source": [
    "### Necessary Packages, Environment Setups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "Sa1iUH1ykUqR"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import transformers\n",
    "\n",
    "from typing import List, Optional, Tuple, Union\n",
    "from transformers import LlamaForCausalLM, LlamaTokenizer, AutoTokenizer\n",
    "from transformers import Trainer, TrainingArguments\n",
    "from itertools import chain\n",
    "from datasets import load_dataset\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "from torch.nn import CrossEntropyLoss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TBzFKWGZkUqR"
   },
   "source": [
    "Please set the correct file path based on your environment.\n",
    "\n",
    "- If you are using Colab, the path may be: `/content/drive/MyDrive/xxxxxx`\n",
    "- If you are using Kaggle, the path may be: `/kaggle/input/xxxxxx`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "mYj0DvSgGmWq"
   },
   "outputs": [],
   "source": [
    "# Please set the correct file path based on your environment.\n",
    "TRAIN_FILE = './data/zh_train.jsonl'\n",
    "VALIDATION_FILE = './data/zh_dev.jsonl'\n",
    "TEST_FILE = './data/zh_test.jsonl'\n",
    "EN_TEST_FILE = './data/en_test.jsonl'\n",
    "MODEL_FOLDER = \"./llama-42m\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ElstHAMjkUqS"
   },
   "source": [
    "Load the model checkpoint into either a GPU or CPU (training will be slow on CPU, but decoding will be fair)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ZF-0tQYDjvPl",
    "outputId": "70706b9e-30c1-470e-f8fc-5ba51c494bc7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device type: cuda\n"
     ]
    }
   ],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Device type: {device}\")\n",
    "\n",
    "model_path = MODEL_FOLDER\n",
    "# Load model from local files\n",
    "model = LlamaForCausalLM.from_pretrained(model_path).to(device)\n",
    "# Load tokenizer from local files\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path, device=device)\n",
    "\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0g81Fac6kUqS"
   },
   "source": [
    "As we can see from the statistics, this model is much smaller than Llama-2 but shares the same decoder-only architecture.\n",
    "\n",
    "\n",
    "😄 **You do not need to check complex details!** We just present the architecture and number of parameters here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "dqxUbbA2u2uc",
    "outputId": "dd426913-1155-44dd-c932-b20683038c00"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LlamaForCausalLM(\n",
      "  (model): LlamaModel(\n",
      "    (embed_tokens): Embedding(32000, 512)\n",
      "    (layers): ModuleList(\n",
      "      (0-7): 8 x LlamaDecoderLayer(\n",
      "        (self_attn): LlamaAttention(\n",
      "          (q_proj): Linear(in_features=512, out_features=512, bias=False)\n",
      "          (k_proj): Linear(in_features=512, out_features=512, bias=False)\n",
      "          (v_proj): Linear(in_features=512, out_features=512, bias=False)\n",
      "          (o_proj): Linear(in_features=512, out_features=512, bias=False)\n",
      "        )\n",
      "        (mlp): LlamaMLP(\n",
      "          (gate_proj): Linear(in_features=512, out_features=1376, bias=False)\n",
      "          (up_proj): Linear(in_features=512, out_features=1376, bias=False)\n",
      "          (down_proj): Linear(in_features=1376, out_features=512, bias=False)\n",
      "          (act_fn): SiLU()\n",
      "        )\n",
      "        (input_layernorm): LlamaRMSNorm((512,), eps=1e-05)\n",
      "        (post_attention_layernorm): LlamaRMSNorm((512,), eps=1e-05)\n",
      "      )\n",
      "    )\n",
      "    (norm): LlamaRMSNorm((512,), eps=1e-05)\n",
      "    (rotary_emb): LlamaRotaryEmbedding()\n",
      "  )\n",
      "  (lm_head): Linear(in_features=512, out_features=32000, bias=False)\n",
      ")\n",
      "#Parameters: 41.69M\n"
     ]
    }
   ],
   "source": [
    "total_para = sum(v.numel() for k, v in model.state_dict().items() if k != 'model.embed_tokens.weight') / 1e6\n",
    "print(model)\n",
    "print(f\"#Parameters: {total_para:.2f}M\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Z6LPYv5KkUqS"
   },
   "source": [
    "### Task 1: Decoding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PxuzZuOrR4mE"
   },
   "source": [
    "\n",
    "If you are familar with the usage of `model.generate()` function in transformer library, please feel free to jump to [Task 1 Playground](#scrollTo=Task_1_Playground).\n",
    "\n",
    "\n",
    "#### 💡Tutorials: model.generate() function.\n",
    "---\n",
    "Minimal example:\n",
    "\n",
    "```python\n",
    "prompt = \"Once upon a time, \" # Input, prefix of generation\n",
    "```\n",
    "\n",
    "**Step 1**: Encode raw text using tokenizer model.\n",
    "```python\n",
    "tokenized_input = tokenizer.encode(prompt, return_tensors='pt').to(device)\n",
    "```\n",
    "\n",
    "**Step 2**: Set decoding hyper-parameters. Get the model output.\n",
    "```python\n",
    "output_ids = model.generate(tokenized_input, do_sample=True, max_new_tokens=300, temperature=0.6)\n",
    "```\n",
    "Important parameters:\n",
    "- `max_new_tokens`: The maximum numbers of tokens to generate, ignoring the number of tokens in the prompt.\n",
    "- `temperature`: The value of temperature used to modulate the next token probabilities. Higher temperature -> generate more diverse text. Lower temperature -> generate more deterministic text.\n",
    "- `do_sample`: `do_sample=False` is using greedy decoing strategy. To enable greedy decoding, we also need to set other sampling parameters `top_p`, `temperature` as `None`.\n",
    "- [If you are interested in other decoding algorithms, please refer to this link for setting parameters.](https://huggingface.co/docs/transformers/v4.44.2/en/main_classes/text_generation#transformers.GenerationConfig)\n",
    "\n",
    "**Step 3**: Convert model outputs into raw text.\n",
    "```python\n",
    "output_text = tokenizer.decode(output_ids[0])\n",
    "```\n",
    "or (when input instances >=1)\n",
    "```python\n",
    "output_text = tokenizer.batch_decode(output_ids)\n",
    "```\n",
    "Important parameters:\n",
    "- Setting `skip_special_tokens=True` will prevent special tokens, such as `<s>`, from appearing in the results..\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tkvkY3LPkUqT"
   },
   "source": [
    "To understand the outputs of each step, let us do a simple generation task step by step! (Note: the base model is only able to produce fluent story text)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "Gf6Q9qcgkUqT"
   },
   "outputs": [],
   "source": [
    "prompt = \"Once upon a time, Stella Lou had a dream.\" # Feel free to use other generation prefix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "BEKNYuWiJKIB",
    "outputId": "3c01550c-3b35-4c2d-c7b7-1fb44af22f5d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[    1,  9038,  2501,   263,   931, 29892,   624,  3547,  4562,   750,\n",
      "           263, 12561, 29889]], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Encode raw text using tokenizer model. Run tokenization and covert strings into token ids in vocabulary.\n",
    "tokenized_input = tokenizer.encode(prompt, return_tensors='pt').to(device)\n",
    "# See the tokenized results.\n",
    "print(tokenized_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "tlmCc71dElxz",
    "outputId": "5cf2d468-1269-492a-f72f-0a92742a139b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====================Token IDs====================\n",
      "tensor([[    1,  9038,  2501,   263,   931, 29892,   624,  3547,  4562,   750,\n",
      "           263, 12561, 29889,  2296,  5131,   304,   367,   263, 12456,   985,\n",
      "         29889,  2296,  5131,   304, 19531,   263,  9560, 10714,   322,   263,\n",
      "           528,  4901, 20844, 29889,  1205,  1183,   471,  2086,  2319,   322,\n",
      "           278, 10714,   471,  2086,  4802, 29889,    13,  6716,  2462, 29892,\n",
      "           624,  3547,  4446,   263,  4802, 29892,   528,  4901, 10714,   297,\n",
      "           263,  3787, 29889,  2296,  4433,   902, 16823,   565,  1183,  1033,\n",
      "           505,   372, 29889,  2439, 16823,  1497,  4874,   322, 18093,   372,\n",
      "           363,   902, 29889,    13,   855,  3547,   471,   577,  9796, 29889,\n",
      "          2296,  1925,   373,   278, 10714,   322,  3252,   381,   839,  2820,\n",
      "         29889,  2296,  7091,   763,   263,  1855, 12456,   985, 29889,    13,\n",
      "          6246,   769, 29892,  1554,  8515,  9559, 29889,   624,  3547,  4687,\n",
      "           304,  4459,   270,   466,  1537, 29889,  2296,  8496, 29915, 29873,\n",
      "          2317,   701,  7812, 29889,  2296,  7091,   763,  1183,   471, 10917,\n",
      "          1076,  2820,   322,  2820, 29889,    13,   855,  3547, 29915, 29879,\n",
      "         16823,  4446,   902,   322,  1497, 29892,   376,   855,  3547, 29892,\n",
      "           366,   817,   304,  2125,   263,  2867, 29889,   887,  1106,   270,\n",
      "           466,  1537,  1213,    13,   855,  3547,  3614,  1283,   278, 10714,\n",
      "           322,  6568,  1623,   373,   278, 11904, 29889,  2296,  5764,   902,\n",
      "          5076,   322,  3614,   263,  6483, 16172, 29889,  2860,   263,  2846,\n",
      "          6233, 29892,  1183,  7091,  2253, 29889,    13,   855,  3547, 25156,\n",
      "           322,  1497, 29892,   376, 29924,   290, 29892,   306, 29915, 29885,\n",
      "          7960,   304,   367,   263, 12456,   985,  1449,  3850,     1]],\n",
      "       device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "# Step 2: Set decoding hyperparameters.\n",
    "\n",
    "# For greedy decoding\n",
    "max_new_tokens = 300\n",
    "do_sample = False  # `do_sample=False` means using greedy decoing strategy. To enable greedy decoding, we also need to set `top_p`, `temperature` as `None`.\n",
    "temperature = None\n",
    "\n",
    "# call generation function model.generate()\n",
    "output_ids = model.generate(\n",
    "    tokenized_input,\n",
    "    max_new_tokens=max_new_tokens,\n",
    "    eos_token_id=1,\n",
    "    do_sample=do_sample,\n",
    "    temperature=temperature,\n",
    "    top_p=None,\n",
    ")\n",
    "\n",
    "# The decoded results are token ids.\n",
    "print(\"=\" * 20 + \"Token IDs\" + \"=\" * 20)\n",
    "print(output_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "uQqRmKrXIYM1",
    "outputId": "124d1235-b532-4f7a-f51a-75f4ee36cb11"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====================Decoded Results====================\n",
      "Once upon a time, Stella Lou had a dream. She wanted to be a princess. She wanted to wear a beautiful dress and a shiny crown. But she was too small and the dress was too big.\n",
      "One day, Stella saw a big, shiny dress in a store. She asked her mom if she could have it. Her mom said yes and bought it for her.\n",
      "Stella was so happy. She put on the dress and twirled around. She felt like a real princess.\n",
      "But then, something strange happened. Stella started to feel dizzy. She couldn't stand up straight. She felt like she was spinning around and around.\n",
      "Stella's mom saw her and said, \"Stella, you need to take a break. You look dizzy.\"\n",
      "Stella took off the dress and lay down on the floor. She closed her eyes and took a deep breath. After a few minutes, she felt better.\n",
      "Stella smiled and said, \"Mom, I'm ready to be a princess again!\"\n"
     ]
    }
   ],
   "source": [
    "# Step 3: Convert model outputs into raw text.\n",
    "# decode token ids into tokens\n",
    "print(\"=\" * 20 + \"Decoded Results\" + \"=\" * 20)\n",
    "# We only have one input instance. So we directly decode the first item of model output, i.e., `output_ids[0]`.\n",
    "print(tokenizer.decode(output_ids[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pRNuqdJkRvxw"
   },
   "source": [
    "#### Another pipeline example: Sampling decoding with temperature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "KCqO5bkl341O",
    "outputId": "bad5340c-a6e7-4ce0-d4eb-389f0ea80c29"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<s> Once upon a time, Stella Lou had a dream. She wanted to be the most successful person in the world. Every day, she would go to the park and play with her friends.\n",
      "One day, Stella saw a big, shiny toy in the park. She wanted it so badly! She asked her friends if she could have it, but they said no.\n",
      "Stella was very sad. She wanted the toy so badly! She decided to take it without asking. She ran away with the toy and hid it in her pocket.\n",
      "But when she got home, she realized the toy was not hers. She felt bad for taking it without asking. She knew it was wrong.\n",
      "Stella decided to return the toy to the park and apologize to the owner. She was very successful in returning the toy and promised to never do it again.\n",
      "The moral of the story is that it is never right to take something that does not belong to you. It is important to be honest and respectful.<s>\n"
     ]
    }
   ],
   "source": [
    "prompt = \"Once upon a time, Stella Lou had a dream.\"\n",
    "\n",
    "# Decoding hyperparameters\n",
    "max_new_tokens = 300\n",
    "do_sample = True\n",
    "# The value of temperature used to modulate the next token probabilities.\n",
    "# Higher temperature -> generate more diverse text. Lower temperature -> generate more deterministic text.\n",
    "temperature = 0.3\n",
    "\n",
    "tokenized_input = tokenizer.encode(prompt, return_tensors=\"pt\").to(device)\n",
    "output_ids = model.generate(\n",
    "    tokenized_input,\n",
    "    max_new_tokens=max_new_tokens,\n",
    "    eos_token_id=1,\n",
    "    do_sample=do_sample,\n",
    "    temperature=temperature,\n",
    ")\n",
    "output_text = tokenizer.decode(output_ids[0])\n",
    "print(output_text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LT7or09aSBhp"
   },
   "source": [
    "#### Task 1 Playground\n",
    "\n",
    "---\n",
    "\n",
    "📚 Task 1: Please generate English stories using various prompts and decoding settings. Please feel free to explore any interesting phenomena, such as the impact of different prompts and the effects of various decoding algorithms and parameters. For example, quantify the text properties using linguistic-driven metrics like story length and Type-Token Ratio (TTR). In addition to objective metrics, you are encouraged to discuss your findings based on subjective case studies.\n",
    "\n",
    "We provide two types of skeleton code: one that takes a single prompt as input and another that can process batched inputs and decoding. Please use the version that best fits your preferences and data types.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "DNvZ5Q6rYeC4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<s> Once upon a time, there was a little girl named Lily. She loved to play outside in the sunshine. One day, she found a shiny rock on the ground. She picked it up and showed it to her mom.\n",
      "\"Look, Mommy! I found a pretty rock!\" Lily said.\n",
      "Her mom smiled and said, \"That's a very nice rock, Lily. But be careful, it's very cold outside. You don't want to get sick.\"\n",
      "Lily nodded and put the rock in her pocket. Later that day, she went to the park to play with her friends. They were playing on the swings when Lily's friend, Timmy, fell off and hurt his arm.\n",
      "Lily remembered what her mom said and told Timmy, \"Don't worry, Timmy. I'll help you. Let's go to your mom and she can help you feel better.\"\n",
      "Timmy smiled and said, \"Thank you, Lily. You're a good friend.\"\n",
      "Lily felt happy that she could help her friend. She learned that it's important to be kind and helpful to others. And she also learned that sometimes, even if things are cold or sharp, they can still be fun to play with.<s>\n"
     ]
    }
   ],
   "source": [
    "# Skeleton Code: Single input (same as previous code blocks)\n",
    "\n",
    "prompt = \"\" # ⬅️ try to construct different prompts.\n",
    "\n",
    "# ⬇️ Try to tune different decoding hyperparameters.\n",
    "# You can also add more hyperparameters like `top_p`, `top_k`.\n",
    "max_new_tokens = 300\n",
    "do_sample = True\n",
    "temperature = 0.3\n",
    "\n",
    "tokenized_input = tokenizer.encode(prompt, return_tensors=\"pt\").to(device)\n",
    "output_ids = model.generate(\n",
    "    tokenized_input,\n",
    "    max_new_tokens=max_new_tokens,\n",
    "    eos_token_id=1,\n",
    "    do_sample=do_sample,\n",
    "    temperature=temperature,\n",
    ")\n",
    "output_text = tokenizer.decode(output_ids[0])\n",
    "print(output_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "80jDK-9uP1IO",
    "outputId": "ceed92d8-c965-4792-b121-ba5bbabab913"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In a forgotten library,Mommy was sitting in the shelves. She was wearing a bright blue dress and looking very happy. She was reading a big book about animals. Suddenly, the shelf shook and the book fell to the floor! It made a loud noise and the book was now broken.\n",
      "Mommy looked at the broken book and smiled. She said, \"Oh no! It's broken.\" She carefully picked up the book and put it back on the shelf. She shook the shelf and said, \"Let's see if we can fix it.\"\n",
      "Mommy and Abigail worked together to fix the book. They taped the shelf back together and taped the broken pieces in place. When they were finished, they looked at the book and saw all the animals and smiled. \n",
      "They read the book together and Abigail asked, \"What is this?\" Abigail said, \"It's a story about a happy bird.\" \n",
      "Mommy laughed and said, \"Yes, it is. And it's good that we fixed it together.\" Abigail smiled and hugged her mommy. They both sat together and enjoyed the story about the happy bird.\n",
      "\n",
      "The key glowed with a faint blue light,a and her big brother Ben.\n",
      "Ben said, \"Let's go on an adventure!\"\n",
      "The two walked together, the key in Ben's hand and the glow of light in Ben's face.\n",
      "They came to a big tree with a mysterious sign on it. Ben said, \"What does it say?\"\n",
      "The key beeped and Ben said, \"It says, 'Only if you use your power.\"\n",
      "Ben put the key into the lock and it opened a secret doorway. Inside the doorway was a bright, blue cage. Ben said, \"Let's go inside.\"\n",
      "The key beeped again, and the door opened. Inside the cage was a giant blue bird with a bright yellow beak. Ben said, \"It's beautiful.\"\n",
      "The bird flapped its wings and flew away. Ben said, \"Come back, bird!\" \n",
      "The key beeped happily and the door opened. Ben and the key went outside and explored the world together.\n",
      "\n",
      "She received a letter from her future self,. She was so happy to receive a letter, but the letter didn't help her.\n",
      "\"It's too hard,\" she said. \"I don't understand what it means.\"\n",
      "Her mom saw her disappointment and said, \"Let's read it together, ok?\"\n",
      "The little girl nodded, so they sat down and read the letter. It was from her best friend, who was very selfish.\n",
      "The letter said, \"Let's play together today.\"\n",
      "The little girl smiled. \"Yes! That would be fun.\"\n",
      "They spent the afternoon playing, but eventually the little girl had to go to bed.\n",
      "The next morning, she ran to her mom and asked, \"Can I have my letter back?\"\n",
      "\"Yes, you may,\" said her mom.\n",
      "The little girl was so excited, she ran to the mailbox and opened it. Inside, there was a big, white envelope.\n",
      "Her mom said, \"This is for you.\"\n",
      "The little girl opened it, and inside was a letter from her friend. She was so happy and she laughed with joy.\n",
      "\"Thank you so much,\" she said to her mom.\n",
      "Her mom smiled and said, \"I knew you would like it.\"\n",
      "\n",
      "The last tree on Earth began to whisper,te. It had a huge horn. Everyone was scared of it, so it was a harmless thing to be scared of. \n",
      "\"Don't be scared, I'm just saying hello,\" said the horn.\n",
      "But the people on Earth were too scared to reply. They ran away in fear. \n",
      "Soon, the horn saw what had happened and it was even bigger and harmless. But the horn was still scared.\n",
      "The horn spoke again, \"Don't be scared. I'm just a harmless horn.\"\n",
      "The people on Earth heard the horn and were brave enough to come closer. They were no longer scared of the horn. \n",
      "The horn smiled, \"Let's be friends, okay?\" \n",
      "The people replied, \"Yes, let's be friends!\"\n",
      "And from that day on, the horn was no longer scared of the people on Earth. It was harmless and everyone was happy.\n",
      "\n",
      "He woke up with the ability to see memories,'s dream. He was excited to take a closer look at the calendar and take in all the special moments of his life. He jumped out of bed and ran to the calendar, eager to discover what it was about.\n",
      "When he arrived at the calendar he was very surprised to see a clear picture of himself on it. He couldn't believe it! He jumped up and down with joy and couldn't wait to explore the other pictures. \n",
      "He wanted to see a different picture, so he started looking for the one he wanted to see. Suddenly he saw a picture of himself, but it wasn't the same. He noticed that there were a lot of different colours and shapes on the calendar. \n",
      "He was amazed and shouted out loud. His mom came to the window and said, \"What happened to you? I heard a loud crash!\" \n",
      "The little boy replied, \"I saw a picture on the calendar and it was different than I thought. I didn't recognize it and it was funny!\" His mom smiled and said, \"Yes, that's the same picture, but it was just your imagination. Now, let's go explore and see what else we can find!\"\n",
      "\n",
      "The clock struck thirteen for the first time,3. Little Billy had been feeling very scared but he knew the time was coming. He was starting to feel scared when his mom came in the room and said, “It’s time for bed, Billy.” Billy was still scared but he got ready for bed.\n",
      "As Billy snuggled into his blankets, he heard a loud ringing sound. He was too scared to get up. His mom came in and said, “It’s okay, it’s just the clock. It won’t strike us when we wake up.” Billy slowly peeked out of his blankets and saw the clock in the corner. He felt relieved.\n",
      "Billy’s mom smiled and said, “See, that wasn’t so scary, was it? The clock ticked to remind you of the day you were born.” Billy smiled and said, “Oh, it was scary!” \n",
      "Billy’s mom hugged him and said, “Let’s go to bed now. Goodnight, Billy.” He happily crawled into bed, feeling safe and secure. Billy knew his mom was always looking out for him and would never let anything scare him.\n",
      "\n",
      "Their shadows moved with a mind of their own,'s ever-so-clean. They were playing outside, feeling the wind on their faces. \n",
      "Suddenly, they heard a voice. It was their mom's voice. \"Aha! We're home,\" she said.\n",
      "Tommy and Jane both rushed to the door. They were so excited to see the big, fluffy dog in the backyard. \n",
      "\"Mom, can we play with him?\" asked Jimmy. \n",
      "Mom laughed. \"Sure, he looks like he wants to play,\" she said.\n",
      "The boys ran outside and started to pet the dog. He was so friendly and wagged his tail. \n",
      "Jimmy and Lucy stopped and stared. \"He's a bit nosy,\" said Jenna. \n",
      "Mom nodded. \"Let's give him a name,\" she said. \n",
      "The boys thought for a moment, and then Mom said, \"Let's call him Shadow.\" \n",
      "The boys giggled and agreed. From then on, Shadow was their best friend.\n",
      "\n",
      "The recipe called for a teaspoon of starlight, who was always looking for help. This one was very popular, and she was always very happy to help out.\n",
      "One day, she went outside and found a little 3 year old boy. He was playing in his garden and he noticed the teaspoon, and he asked his dad if he could have it. His dad said yes and the little boy was very excited.\n",
      "The little boy was so happy he started to dance and jump around. He was so excited he started to laugh and smile. His dad laughed too, and they both joined in the fun.\n",
      "The little boy played with the teaspoon until night, and when it was time to go to bed, he thanked his dad for the special gift. From then on, the little boy would always look for the teaspoon, and every time, he would laugh and smile.\n",
      "\n",
      "The train platform was invisible to everyone else,! It was so hidden, nobody could see it!\n",
      "The train conductor looked around and noticed that there was a small girl standing at the center. She was wearing a yellow dress and had a big smile on her face.\n",
      "The conductor smiled and asked her, \"Do you recognize me?\" The girl nodded and pointed to the train.\n",
      "The conductor looked around and finally found the girl's mother. He pointed to the train and said, \"Let's go!\"\n",
      "The girl and her mother got on the train and the conductor took them to their destination. The girl was so excited to see all of the new things.\n",
      "At the end of the day, the girl said goodbye to the conductor and thanked him for recognizing her. The conductor smiled and waved as they drove away.\n",
      "\n",
      "'ly skipped around the garden. He was wearing his favorite red sweater that his mom had bought for him and it made him feel so warm and cosy.\n",
      "Momma said to him, \"Let's take a walk together, please?\"\n",
      "Darean smiled and nodded. They started walking around the garden and the two of them talked about all kinds of things.\n",
      "Momma said, \"We should find a nice place to sit and rest. Let's go to the garden bench over there.\"\n",
      "Dareommie was so excited, he couldn't contain his joy. They got to the bench and Momma told him, \"This bench is perfect for us. Let's stay here for a few minutes, okay?\"\n",
      "DareBoo enjoyed the view from the bench and he started to skip. He looked around and saw the beautiful flowers and the little birds singing in the trees. After a few minutes, Momma said, \"It's time to go. We should hurry back to the house, Dare.\"\n",
      "Dareomma was so happy and the two of them skipped back to the house together.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Skeleton Code: Bacthed input-output\n",
    "\n",
    "prompts = [\n",
    "    \"In a forgotten library,\",\n",
    "    \"The key glowed with a faint blue light,\",\n",
    "    \"She received a letter from her future self,\",\n",
    "    \"The last tree on Earth began to whisper,\",\n",
    "    \"He woke up with the ability to see memories,\",\n",
    "    \"The clock struck thirteen for the first time,\",\n",
    "    \"Their shadows moved with a mind of their own,\",\n",
    "    \"The recipe called for a teaspoon of starlight,\",\n",
    "    \"The train platform was invisible to everyone else,\",\n",
    "    \"\"\n",
    "] # ⬅️ try to construct different prompts.\n",
    "\n",
    "batch_size = 10 # If you have multiple data inputs, please control the batch size to prevent out-of-memory issues.\n",
    "\n",
    "# ⬇️ Try to tune different decoding hyperparameters.\n",
    "# You can also add more hyperparameters like `top_p`, `top_k`.\n",
    "max_new_tokens = 300\n",
    "do_sample = True\n",
    "temperature = 0.9\n",
    "\n",
    "for i in range(0, len(prompts), batch_size):\n",
    "    batch_input = prompts[i:i+batch_size]\n",
    "    tokenized_input = tokenizer(batch_input, return_tensors=\"pt\", padding=True).to(device)\n",
    "\n",
    "    # For decoder-only models, batched inputs of model.generate() should be in the format of input_ids.\n",
    "    output_ids = model.generate(\n",
    "        tokenized_input[\"input_ids\"],\n",
    "        max_new_tokens=max_new_tokens,\n",
    "        eos_token_id=1,\n",
    "        do_sample=do_sample,\n",
    "        temperature=temperature,\n",
    "    )\n",
    "    output_text = tokenizer.batch_decode(output_ids, skip_special_tokens=True)\n",
    "\n",
    "    for idx, result in enumerate(output_text):\n",
    "        print(f\"{result}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rhfIINEMSIjp"
   },
   "source": [
    "#### What about other languages?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pYlvKScxkUqT"
   },
   "source": [
    "Oops! This English language model cannot generate stories in other languages!\n",
    "\n",
    "Why? Let us evaluate the perplexity of different languages in the next task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "C0jb_ZqW_ixo",
    "outputId": "e5f718d0-8025-438e-e5d5-79eb2ce7932c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<s> 从前有一只小兔子乖乖ons were playing in the garden. They were having a lot of fun. Suddenly, a big, scary dog came running towards them. The dog was very angry and growled loudly.\n",
      "Little Timmy was scared and started to cry. His mom came running out of the house and shouted at the dog. The dog stopped and looked at Timmy.\n",
      "\"It's ok, Timmy,\" said his mom. \"Let's go inside and get you out of the garden.\"\n",
      "Timmy and his mom went inside and Timmy felt safe. His mom gave him a big hug and said, \"Let's go back outside and play.\"\n",
      "Timmy smiled and they both went back outside. He was happy to be back in the garden and play. He was no longer scared.<s>\n"
     ]
    }
   ],
   "source": [
    "prompt = \"从前有一只小兔子乖乖\"\n",
    "\n",
    "# Decoding hyperparameters\n",
    "max_new_tokens = 300\n",
    "do_sample = True\n",
    "temperature = 0.3\n",
    "\n",
    "tokenized_input = tokenizer.encode(prompt, return_tensors=\"pt\").to(device)\n",
    "output_ids = model.generate(\n",
    "    tokenized_input,\n",
    "    max_new_tokens=max_new_tokens,\n",
    "    eos_token_id=1,\n",
    "    do_sample=do_sample,\n",
    "    temperature=temperature,\n",
    ")\n",
    "output_text = tokenizer.decode(output_ids[0])\n",
    "print(output_text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DYst_QWTkUqU"
   },
   "source": [
    "### Task 2: Perplexity Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "T6NxwPeaBzOB"
   },
   "source": [
    "#### Background\n",
    "\n",
    "---\n",
    "\n",
    "The perplexity serves as a key metric for evaluating language models. It quantifies how well a model predicts a sample, with lower perplexity indicating better performance. For a tokenized sequence $X = (x_0, x_1, \\dots, x_t)$, the perplexity is defined mathematically as:\n",
    "\n",
    "$$\\text{Perplexity}(X) = \\exp \\left( -\\frac{1}{t} \\sum_{i=1}^t \\log p_\\theta (x_i | x_{<i}) \\right)$$\n",
    "\n",
    "Here, $p_\\theta(x_i | x_{<i})$ represents the probability of a token $ x_i $ given its preceding tokens, and the formulation incorporates the average log probability across the sequence.\n",
    "\n",
    "---\n",
    "\n",
    "⚠️ Please make sure to **run the following cell first** to define the evaluation function.\n",
    "\n",
    "😄 **You do not need to check these complex details! Too hard for beginners!** However, if you are interested, you can compare the following code with the explanations above to better understand how to implement PPL evaluation using PyTorch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "bEIdmBRsJaP9"
   },
   "outputs": [],
   "source": [
    "# The following code was adapted from the `evaluate` library. Licensed under the Apache License, Version 2.0 (the \"License\").\n",
    "# We modify them to avoid causing serious memory issues in the Colab environment.\n",
    "\n",
    "def compute_ppl(\n",
    "        model, tokenizer, inputs, device, batch_size: int = 16, add_start_token: bool = True, max_length=None\n",
    "):\n",
    "\n",
    "    if device is not None:\n",
    "        assert device in [\"gpu\", \"cpu\", \"cuda\"], \"device should be either gpu or cpu.\"\n",
    "        if device == \"gpu\":\n",
    "            device = \"cuda\"\n",
    "    else:\n",
    "        device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "    # if batch_size > 1 (which generally leads to padding being required), and\n",
    "    # if there is not an already assigned pad_token, assign an existing\n",
    "    # special token to also be the padding token\n",
    "    if tokenizer.pad_token is None and batch_size > 1:\n",
    "        existing_special_tokens = list(tokenizer.special_tokens_map_extended.values())\n",
    "        # check that the model already has at least one special token defined\n",
    "        assert (\n",
    "            len(existing_special_tokens) > 0\n",
    "        ), \"If batch_size > 1, model must have at least one special token to use for padding. Please use a different model or set batch_size=1.\"\n",
    "        # assign one of the special tokens to also be the pad token\n",
    "        tokenizer.add_special_tokens({\"pad_token\": existing_special_tokens[0]})\n",
    "\n",
    "    if add_start_token and max_length:\n",
    "        # leave room for <BOS> token to be added:\n",
    "        assert (\n",
    "            tokenizer.bos_token is not None\n",
    "        ), \"Input model must already have a BOS token if using add_start_token=True. Please use a different model, or set add_start_token=False\"\n",
    "        max_tokenized_len = max_length - 1\n",
    "    else:\n",
    "        max_tokenized_len = max_length\n",
    "\n",
    "    encodings = tokenizer(\n",
    "        inputs,\n",
    "        add_special_tokens=False,\n",
    "        padding=True,\n",
    "        truncation=True if max_tokenized_len else False,\n",
    "        max_length=max_tokenized_len,\n",
    "        return_tensors=\"pt\",\n",
    "        return_attention_mask=True,\n",
    "    )\n",
    "\n",
    "    encoded_texts = encodings[\"input_ids\"]\n",
    "    attn_masks = encodings[\"attention_mask\"]\n",
    "\n",
    "    # check that each input is long enough:\n",
    "    if add_start_token:\n",
    "        assert torch.all(torch.ge(attn_masks.sum(1), 1)), \"Each input text must be at least one token long.\"\n",
    "    else:\n",
    "        assert torch.all(\n",
    "            torch.ge(attn_masks.sum(1), 2)\n",
    "        ), \"When add_start_token=False, each input text must be at least two tokens long. Run with add_start_token=True if inputting strings of only one token, and remove all empty input strings.\"\n",
    "\n",
    "    ppls = []\n",
    "    loss_fct = CrossEntropyLoss(reduction=\"none\")\n",
    "\n",
    "    for start_index in tqdm(range(0, len(encoded_texts), batch_size)):\n",
    "        end_index = min(start_index + batch_size, len(encoded_texts))\n",
    "        encoded_batch = encoded_texts[start_index:end_index].to(device)\n",
    "        attn_mask = attn_masks[start_index:end_index].to(device)\n",
    "\n",
    "        if add_start_token:\n",
    "            bos_tokens_tensor = torch.tensor([[tokenizer.bos_token_id]] * encoded_batch.size(dim=0)).to(device)\n",
    "            encoded_batch = torch.cat([bos_tokens_tensor, encoded_batch], dim=1)\n",
    "            attn_mask = torch.cat(\n",
    "                [torch.ones(bos_tokens_tensor.size(), dtype=torch.int64).to(device), attn_mask], dim=1\n",
    "            )\n",
    "\n",
    "        labels = encoded_batch\n",
    "\n",
    "        with torch.no_grad():\n",
    "            out_logits = model(encoded_batch, attention_mask=attn_mask).logits\n",
    "\n",
    "            shift_logits = out_logits[..., :-1, :].contiguous()\n",
    "            shift_labels = labels[..., 1:].contiguous()\n",
    "            shift_attention_mask_batch = attn_mask[..., 1:].contiguous()\n",
    "\n",
    "            perplexity_batch = torch.exp(\n",
    "                (loss_fct(shift_logits.transpose(1, 2), shift_labels) * shift_attention_mask_batch).sum(1)\n",
    "                / shift_attention_mask_batch.sum(1)\n",
    "            )\n",
    "\n",
    "            ppls += perplexity_batch.tolist()\n",
    "\n",
    "    del encoded_batch, attn_mask\n",
    "    if device == \"cuda\":\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "    return {\"perplexities\": ppls, \"mean_perplexity\": sum(ppls)/float(len(ppls))}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9TscJWpxB2XH"
   },
   "source": [
    "\n",
    "#### 💡Tutorials: compute_ppl() function.\n",
    "\n",
    "---\n",
    "Minimal example:\n",
    "\n",
    "```python\n",
    "test_dataset = [\"Once upon a time,\"]\n",
    "\n",
    "compute_ppl(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    device=device,\n",
    "    inputs=test_dataset,\n",
    "    batch_size = 16\n",
    ")\n",
    "```\n",
    "\n",
    "Important parameters:\n",
    "- `inputs`: list of input text, each separate text snippet is one list entry.\n",
    "- `batch_size`: the batch size to run evaluations.\n",
    "\n",
    "Returns:\n",
    "- `perplexity`: `{\"perplexities\": [x.x, x.x, ...], \"mean_perplexity\": x.x}` dictionary containing the perplexity scores for the texts in the input list, as well as the mean perplexity. .\n",
    "\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2yDr5pIVCHCP"
   },
   "source": [
    "#### Task 2 Playground\n",
    "\n",
    "---\n",
    "\n",
    "📚 Task 2: Evaluate the perplexity. Ensure that you evaluate both the English and Chinese test data we provided. You are encouraged to collect more diverse text data and discuss your findings regarding the language understanding capacity of the base model.\n",
    "\n",
    "\n",
    "Note: If you want to reuse the evaluation codes for JSONL data, please structure the content as follows:\n",
    "```json\n",
    "{\"text\": \"one data\"}\n",
    "{\"text\": \"two data.\"}\n",
    "...\n",
    "```\n",
    "**You may find that the PPL value for Chinese text is significantly higher than that for English text. This is evidence that the base model cannot generate a Chinese story at the end of the last task.**\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 66,
     "referenced_widgets": [
      "35d813e7b32f44e49d38155e4f950c86",
      "4245c28c8eec4edd921b1755ad1ad45b",
      "2c286ad6343a475d8516ac4779b208ac",
      "62beee732f344d1fabf7e213930ad775",
      "037842bf542143038269aedb14c51c0d",
      "7f004def454c46f4a489d83a265577f6",
      "5407caa8f2df48179033c1d0d6daebfe",
      "fa04910012354222b3f1a49ed5e33203",
      "b45c1921cebe42418c9bac889be111de",
      "645cdc1efc3344dc8625f9ed98ee4433",
      "b2f56d482fdd43dea48c9f649680e11b"
     ]
    },
    "id": "QSWMQtwDSdIm",
    "outputId": "0841a9fc-fe40-499b-aae7-15d202149077"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dfdacaae6a9f46bb84d419b92ec31115",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Perplexity: 10.68\n"
     ]
    }
   ],
   "source": [
    "# Skeleton Code: Evaluate the perplexity (PPL) on a list of raw text.\n",
    "\n",
    "test_dataset = [\"Once upon a time,\", \"Tom is a cute kitty.\"] # ⬅️ you can use your examples / or read from raw text file\n",
    "\n",
    "results = compute_ppl(model=model, tokenizer=tokenizer, device=device, inputs=test_dataset, batch_size = 16)\n",
    "dataset_ppl = results['mean_perplexity']\n",
    "print(f\"Perplexity: {dataset_ppl:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 115,
     "referenced_widgets": [
      "2df92583f44640f38112db08ea30cc5e",
      "9ef56642df394267850e820ae578d3d7",
      "fb11f7b6a113420cbf9353c79f434ad5",
      "b039f8470ffa4535ad065fc0eda886fd",
      "d2f6deec117a4358ad064db89f727bc1",
      "459a84f78164433884ba5f470a9e1417",
      "19620102f85d4c6a8b48e1078a180e95",
      "8995660b5da643a495cc5d8d23104913",
      "2ae1f05679b84ac9bac39c77485e6b01",
      "e063f6ac5f4a44b88271e700e985185a",
      "325cd0c8a932466d9c115ffba2904bba",
      "1a50911f63024bf59da9873dd6d1a912",
      "e9b6ee204f4a457ab9863c0f3baf4864",
      "40a0c17e7fde4bf2862ecda678d5e963",
      "e0cdab58ab0c4936878db9d524c8726c",
      "b2143d465f7047629cdfe9777e3a286b",
      "e56cd9f3c4124896a1f04d55b19888ea",
      "67c58390dc914bb6a8db182ab1943157",
      "76f64cb2e08e453d977b20150745fe41",
      "35fe5e0e1ce14594b643f9679ea2b7f3",
      "b5769bc0ec3e4662b08fc27d8dda40c8",
      "91f68453eca74af8804b1871a20a0b26"
     ]
    },
    "id": "4a_qx_9LLHYI",
    "outputId": "a66756f4-6ae8-4d8b-e1dd-7697d6e75966",
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "15549c2403b04268b9cc185ff962bda6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/32 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(English Text) Test Perplexity: 4.14\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "85b8d3e85e1e45948d5a870770906f97",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/32 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Chinese Text) Test Perplexity: 70030.33\n"
     ]
    }
   ],
   "source": [
    "# English test set.\n",
    "data_file = EN_TEST_FILE\n",
    "test_dataset = load_dataset('json', data_files={'test': data_file})[\"test\"]\n",
    "text_data = list(test_dataset[\"text\"])  # 转换为字符串列表\n",
    "\n",
    "results = compute_ppl(model=model, tokenizer=tokenizer, device=device, inputs=text_data, batch_size=32)\n",
    "dataset_ppl = results['mean_perplexity']\n",
    "print(f\"(English Text) Test Perplexity: {dataset_ppl:.2f}\")\n",
    "\n",
    "# Chinese test set.\n",
    "data_file = TEST_FILE\n",
    "test_dataset = load_dataset('json', data_files={'test': data_file})[\"test\"]\n",
    "text_data = list(test_dataset[\"text\"])  # 转换为字符串列表\n",
    "\n",
    "results = compute_ppl(model=model, tokenizer=tokenizer, device=device, inputs=text_data, batch_size=32)\n",
    "dataset_ppl = results['mean_perplexity']\n",
    "print(f\"(Chinese Text) Test Perplexity: {dataset_ppl:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "MVELmXzJP7GS"
   },
   "outputs": [],
   "source": [
    "# 🚨 Release gpu cache before training the model\n",
    "if device == \"cuda\":\n",
    "    torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_gilWue9kUqU"
   },
   "source": [
    "### Task 3: Continual Pre-training (in Chinese or in another language you are proficient in)\n",
    "\n",
    "Currently, our base English LM is proficient in English but lacks the capability to generate or comprehend other languages (e.g., Chinese). The objective of this task is to enhance a base English LM by continually pre-training it with text in another language. This process aims to enable the model to understand and generate mini-story in another language.\n",
    "\n",
    "We have provided 10,000 Chinese training samples. The training process for any language is the same. We have included useful resource links (in Assignment description PDF) to help you create additional data. If you encounter any issues in creating a dataset in another language, please do not hesitate to contact us.\n",
    "\n",
    "We have implemented data preprocessing and the training pipeline, so you are not required to optimize these components. Instead, focus on tuning the training hyperparameters and observe the changes in model performance.\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "⚠️ Please **make sure to run the following cell first to pre-process data**.\n",
    "\n",
    "😄 You do not need to check the details of whole pipeline construction! Please pay attention to the hyper-parameters of `trainer`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dzYCSeGVkUqT"
   },
   "source": [
    "#### Preprocess Data\n",
    "Here, we preprocess (tokenize and group) the text for the subsequent evaluation and pre-training phases."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HhQN9DpQkUqT"
   },
   "source": [
    "Load prepared Chinese dataset from Google drive (or local disk)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "SiGEfoUMkUqT",
    "outputId": "66d7b579-4295-41e9-9b75-60e2c941d378"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['text'],\n",
      "        num_rows: 10000\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['text'],\n",
      "        num_rows: 500\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['text'],\n",
      "        num_rows: 1000\n",
      "    })\n",
      "})\n",
      "从前，有一个小男孩叫蒂米，他喜欢在外面玩耍。有一天，他和妈妈去了公园。他们在秋千和滑梯上玩得很开心。但是，然后他的手臂开始发痒。\n",
      "“妈妈，我的手臂在痒！”蒂米说。\n",
      "“我们回家抹点乳液吧。”他妈妈回答。\n",
      "当他们走回家时，蒂米看到人行道上有一个大水坑。他想跳进去，但妈妈说：“快点，蒂米，我们要在下雨之前回家。”\n",
      "\n",
      "回到家后，蒂米的妈妈给他手臂抹了些乳液，止住了瘙痒。“好多了。”蒂米说。\n",
      "那天晚些时候，雨下得很大。蒂米很高兴他们及时回家了。“谢谢你赶路，妈妈。”他说。\n"
     ]
    }
   ],
   "source": [
    "chinese_dataset = load_dataset('json', data_files={'train': TRAIN_FILE, 'validation':VALIDATION_FILE, 'test': TEST_FILE})\n",
    "print(chinese_dataset)\n",
    "print(chinese_dataset[\"test\"][1][\"text\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uAcSghJgkUqT"
   },
   "source": [
    "We tokenize the raw text using Llama-2's tokenizer and group the tokenized text as inputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device type: cuda\n"
     ]
    }
   ],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Device type: {device}\")\n",
    "\n",
    "model_path = MODEL_FOLDER\n",
    "# Load model from local files\n",
    "model = LlamaForCausalLM.from_pretrained(model_path).to(device)\n",
    "# Load tokenizer from local files\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path, device=device)\n",
    "\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "id": "tUTCHIqEWJRL"
   },
   "outputs": [],
   "source": [
    "block_size = 512\n",
    "\n",
    "def group_texts(examples):\n",
    "    concatenated_examples = {k: list(chain(*examples[k])) for k in examples.keys()}\n",
    "    total_length = len(concatenated_examples[list(examples.keys())[0]])\n",
    "    if total_length >= block_size:\n",
    "        total_length = (total_length // block_size) * block_size\n",
    "    result = {\n",
    "        k: [t[i : i + block_size] for i in range(0, total_length, block_size)]\n",
    "        for k, t in concatenated_examples.items()\n",
    "    }\n",
    "    result[\"labels\"] = result[\"input_ids\"].copy()\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "dXoXO29hWY-v",
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "27c17ab1fea64cf6a8fa32eeb29e8b3f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=4):   0%|          | 0/10000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "NameError",
     "evalue": "name 'tokenizer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRemoteTraceback\u001b[0m                           Traceback (most recent call last)",
      "\u001b[1;31mRemoteTraceback\u001b[0m: \n\"\"\"\nTraceback (most recent call last):\n  File \"C:\\Users\\yuank\\.conda\\envs\\TinyLlama\\lib\\site-packages\\multiprocess\\pool.py\", line 125, in worker\n    result = (True, func(*args, **kwds))\n  File \"C:\\Users\\yuank\\.conda\\envs\\TinyLlama\\lib\\site-packages\\datasets\\utils\\py_utils.py\", line 586, in _write_generator_to_queue\n    for i, result in enumerate(func(**kwargs)):\n  File \"C:\\Users\\yuank\\.conda\\envs\\TinyLlama\\lib\\site-packages\\datasets\\arrow_dataset.py\", line 3681, in _map_single\n    for i, batch in iter_outputs(shard_iterable):\n  File \"C:\\Users\\yuank\\.conda\\envs\\TinyLlama\\lib\\site-packages\\datasets\\arrow_dataset.py\", line 3631, in iter_outputs\n    yield i, apply_function(example, i, offset=offset)\n  File \"C:\\Users\\yuank\\.conda\\envs\\TinyLlama\\lib\\site-packages\\datasets\\arrow_dataset.py\", line 3554, in apply_function\n    processed_inputs = function(*fn_args, *additional_args, **fn_kwargs)\n  File \"C:\\Users\\yuank\\AppData\\Local\\Temp\\ipykernel_20192\\3564456195.py\", line 1, in <lambda>\nNameError: name 'tokenizer' is not defined\n\"\"\"",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[28], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m tokenized_zh_datasets \u001b[38;5;241m=\u001b[39m \u001b[43mchinese_dataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmap\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mexamples\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mexamples\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtext\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatched\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_proc\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m4\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mremove_columns\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtext\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      2\u001b[0m lm_datasets \u001b[38;5;241m=\u001b[39m tokenized_zh_datasets\u001b[38;5;241m.\u001b[39mmap(\n\u001b[0;32m      3\u001b[0m     group_texts,\n\u001b[0;32m      4\u001b[0m     batched\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m      5\u001b[0m     batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m512\u001b[39m,\n\u001b[0;32m      6\u001b[0m     num_proc\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m4\u001b[39m,\n\u001b[0;32m      7\u001b[0m )\n",
      "File \u001b[1;32m~\\.conda\\envs\\TinyLlama\\lib\\site-packages\\datasets\\dataset_dict.py:954\u001b[0m, in \u001b[0;36mDatasetDict.map\u001b[1;34m(self, function, with_indices, with_rank, with_split, input_columns, batched, batch_size, drop_last_batch, remove_columns, keep_in_memory, load_from_cache_file, cache_file_names, writer_batch_size, features, disable_nullable, fn_kwargs, num_proc, desc, try_original_type)\u001b[0m\n\u001b[0;32m    951\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m with_split:\n\u001b[0;32m    952\u001b[0m     function \u001b[38;5;241m=\u001b[39m bind(function, split)\n\u001b[1;32m--> 954\u001b[0m dataset_dict[split] \u001b[38;5;241m=\u001b[39m \u001b[43mdataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmap\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    955\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfunction\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfunction\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    956\u001b[0m \u001b[43m    \u001b[49m\u001b[43mwith_indices\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mwith_indices\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    957\u001b[0m \u001b[43m    \u001b[49m\u001b[43mwith_rank\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mwith_rank\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    958\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_columns\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_columns\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    959\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatched\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatched\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    960\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    961\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdrop_last_batch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdrop_last_batch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    962\u001b[0m \u001b[43m    \u001b[49m\u001b[43mremove_columns\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mremove_columns\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    963\u001b[0m \u001b[43m    \u001b[49m\u001b[43mkeep_in_memory\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkeep_in_memory\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    964\u001b[0m \u001b[43m    \u001b[49m\u001b[43mload_from_cache_file\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mload_from_cache_file\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    965\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcache_file_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_file_names\u001b[49m\u001b[43m[\u001b[49m\u001b[43msplit\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    966\u001b[0m \u001b[43m    \u001b[49m\u001b[43mwriter_batch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mwriter_batch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    967\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfeatures\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfeatures\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    968\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdisable_nullable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdisable_nullable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    969\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfn_kwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfn_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    970\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_proc\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_proc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    971\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdesc\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdesc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    972\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtry_original_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtry_original_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    973\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    975\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m with_split:\n\u001b[0;32m    976\u001b[0m     function \u001b[38;5;241m=\u001b[39m function\u001b[38;5;241m.\u001b[39mfunc\n",
      "File \u001b[1;32m~\\.conda\\envs\\TinyLlama\\lib\\site-packages\\datasets\\arrow_dataset.py:562\u001b[0m, in \u001b[0;36mtransmit_format.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    555\u001b[0m self_format \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m    556\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtype\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_format_type,\n\u001b[0;32m    557\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mformat_kwargs\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_format_kwargs,\n\u001b[0;32m    558\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcolumns\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_format_columns,\n\u001b[0;32m    559\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moutput_all_columns\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_output_all_columns,\n\u001b[0;32m    560\u001b[0m }\n\u001b[0;32m    561\u001b[0m \u001b[38;5;66;03m# apply actual function\u001b[39;00m\n\u001b[1;32m--> 562\u001b[0m out: Union[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDataset\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDatasetDict\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m func(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    563\u001b[0m datasets: \u001b[38;5;28mlist\u001b[39m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDataset\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(out\u001b[38;5;241m.\u001b[39mvalues()) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(out, \u001b[38;5;28mdict\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m [out]\n\u001b[0;32m    564\u001b[0m \u001b[38;5;66;03m# re-apply format to the output\u001b[39;00m\n",
      "File \u001b[1;32m~\\.conda\\envs\\TinyLlama\\lib\\site-packages\\datasets\\arrow_dataset.py:3316\u001b[0m, in \u001b[0;36mDataset.map\u001b[1;34m(self, function, with_indices, with_rank, input_columns, batched, batch_size, drop_last_batch, remove_columns, keep_in_memory, load_from_cache_file, cache_file_name, writer_batch_size, features, disable_nullable, fn_kwargs, num_proc, suffix_template, new_fingerprint, desc, try_original_type)\u001b[0m\n\u001b[0;32m   3313\u001b[0m os\u001b[38;5;241m.\u001b[39menviron \u001b[38;5;241m=\u001b[39m prev_env\n\u001b[0;32m   3314\u001b[0m logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSpawning \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnum_proc\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m processes\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m-> 3316\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m rank, done, content \u001b[38;5;129;01min\u001b[39;00m iflatmap_unordered(\n\u001b[0;32m   3317\u001b[0m     pool, Dataset\u001b[38;5;241m.\u001b[39m_map_single, kwargs_iterable\u001b[38;5;241m=\u001b[39munprocessed_kwargs_per_job\n\u001b[0;32m   3318\u001b[0m ):\n\u001b[0;32m   3319\u001b[0m     check_if_shard_done(rank, done, content)\n\u001b[0;32m   3321\u001b[0m pool\u001b[38;5;241m.\u001b[39mclose()\n",
      "File \u001b[1;32m~\\.conda\\envs\\TinyLlama\\lib\\site-packages\\datasets\\utils\\py_utils.py:626\u001b[0m, in \u001b[0;36miflatmap_unordered\u001b[1;34m(pool, func, kwargs_iterable)\u001b[0m\n\u001b[0;32m    623\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    624\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m pool_changed:\n\u001b[0;32m    625\u001b[0m         \u001b[38;5;66;03m# we get the result in case there's an error to raise\u001b[39;00m\n\u001b[1;32m--> 626\u001b[0m         [async_result\u001b[38;5;241m.\u001b[39mget(timeout\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.05\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m async_result \u001b[38;5;129;01min\u001b[39;00m async_results]\n",
      "File \u001b[1;32m~\\.conda\\envs\\TinyLlama\\lib\\site-packages\\datasets\\utils\\py_utils.py:626\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    623\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    624\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m pool_changed:\n\u001b[0;32m    625\u001b[0m         \u001b[38;5;66;03m# we get the result in case there's an error to raise\u001b[39;00m\n\u001b[1;32m--> 626\u001b[0m         [\u001b[43masync_result\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.05\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m async_result \u001b[38;5;129;01min\u001b[39;00m async_results]\n",
      "File \u001b[1;32m~\\.conda\\envs\\TinyLlama\\lib\\site-packages\\multiprocess\\pool.py:774\u001b[0m, in \u001b[0;36mApplyResult.get\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    772\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_value\n\u001b[0;32m    773\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 774\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_value\n",
      "\u001b[1;31mNameError\u001b[0m: name 'tokenizer' is not defined"
     ]
    }
   ],
   "source": [
    "tokenized_zh_datasets = chinese_dataset.map(lambda examples: tokenizer(examples[\"text\"]), batched=True, num_proc=4, remove_columns=[\"text\"])\n",
    "lm_datasets = tokenized_zh_datasets.map(\n",
    "    group_texts,\n",
    "    batched=True,\n",
    "    batch_size=512,\n",
    "    num_proc=4,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-DUTC5J7nSZt"
   },
   "source": [
    "#### 💡Tutorials: TrainingArguments().\n",
    "\n",
    "**Important Training Hyper-parameters**\n",
    "- learning_rate: The initial learning rate for optimizer.\n",
    "- num_train_epochs: Total number of training epochs to perform (if not an integer, will perform the decimal part percents of the last epoch before stopping training).\n",
    "- *_strategy: The evaluation/saving strategy to adopt during training. Possible values are:\n",
    "    - `\"no\"`: No evaluation/saving is done during training.\n",
    "    - `\"steps\"`: Evaluation/saving is done (and logged) every `eval_steps`.\n",
    "    - `\"epoch\"`: Evaluation/saving is done at the end of each epoch.\n",
    "- per_device_train_batch_size: The batch size per GPU/XPU/TPU/MPS/NPU core/CPU for training.\n",
    "- per_device_eval_batch_size: The batch size per GPU/XPU/TPU/MPS/NPU core/CPU for evaluation.\n",
    "- save_total_limit: If a value is passed, will limit the total amount of checkpoints.\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "If you do not understand `AdamW` optimizer and learning scheduler, you may use default settings.\n",
    "\n",
    "**Optimizer Hyper-parameters**\n",
    "- weight_decay: The weight decay to apply (if not zero) to all layers except all bias and LayerNorm weights in [`AdamW`] optimizer.\n",
    "- adam_beta1: The beta1 hyperparameter for the [`AdamW`] optimizer.\n",
    "- adam_beta2: The beta2 hyperparameter for the [`AdamW`] optimizer.\n",
    "\n",
    "**Learning schedule**\n",
    "- lr_scheduler: The scheduler type to use.\n",
    "- warmup_ratio: Ratio of total training steps used for a linear warmup from 0 to `learning_rate`.\n",
    "\n",
    "[Explore more parameters here](https://huggingface.co/docs/transformers/v4.44.2/en/main_classes/trainer#transformers.TrainingArguments)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "R9OJF3VNhrGK"
   },
   "source": [
    "#### Task 3 Playground\n",
    "\n",
    "---\n",
    "\n",
    "📚 Please just run the following code to do continual pre-training. Please try your best to tune the hyperparameters or collect more data to improve model performance.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Lf6bS7lnbPMM"
   },
   "outputs": [],
   "source": [
    "# =========Pre-training hyperparameters, please feel free to tune them~=========\n",
    "# =Important=\n",
    "lr = 1e-4\n",
    "epochs = 8\n",
    "save_steps=200\n",
    "strategy=\"steps\"\n",
    "train_bsz = 32 # reduce batch size if you encountered out-of-memory errors.\n",
    "eval_bsz = 16\n",
    "\n",
    "# If you do not understand AdamW optimizer and learning scheduler, you may use default settings.\n",
    "# =Optimizer=\n",
    "optimizer = \"adamw_torch\"\n",
    "weight_decay = 0.01\n",
    "adam_beta1 = 0.9\n",
    "adam_beta2 = 0.98\n",
    "# =Learning scheduler=\n",
    "lr_scheduler = \"linear\"\n",
    "warmup_ratio = 0.01\n",
    "# =========End of pre-training hyperparameters=========\n",
    "\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    \"llama-42m-zh-fairytales\",\n",
    "    evaluation_strategy = strategy,\n",
    "    eval_steps=save_steps,\n",
    "    save_strategy = strategy,\n",
    "    save_steps=save_steps,\n",
    "    logging_strategy=\"steps\",\n",
    "    logging_steps = 10,\n",
    "    learning_rate=lr,\n",
    "    weight_decay=weight_decay,\n",
    "    seed=42,\n",
    "    per_device_train_batch_size=train_bsz,\n",
    "    per_device_eval_batch_size=eval_bsz,\n",
    "    save_total_limit=1,\n",
    "    optim = optimizer,\n",
    "    lr_scheduler_type = lr_scheduler,\n",
    "    adam_beta1 = adam_beta1,\n",
    "    adam_beta2 = adam_beta2,\n",
    "    warmup_ratio = warmup_ratio,\n",
    "    num_train_epochs = epochs,\n",
    "    report_to=None\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=lm_datasets[\"train\"],\n",
    "    eval_dataset=lm_datasets[\"validation\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 515
    },
    "id": "KCIR43fZcAa9",
    "outputId": "adecb9fd-1375-42d9-f7b2-60b91e4dc1de"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2000' max='2000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [2000/2000 12:13, Epoch 8/8]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>3.298300</td>\n",
       "      <td>3.243898</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>1.910000</td>\n",
       "      <td>1.898938</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>1.548700</td>\n",
       "      <td>1.571377</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>1.381600</td>\n",
       "      <td>1.436444</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>1.313700</td>\n",
       "      <td>1.372024</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>1.252300</td>\n",
       "      <td>1.322885</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1400</td>\n",
       "      <td>1.179800</td>\n",
       "      <td>1.296618</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1600</td>\n",
       "      <td>1.129800</td>\n",
       "      <td>1.278112</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1800</td>\n",
       "      <td>1.088200</td>\n",
       "      <td>1.270051</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>1.106700</td>\n",
       "      <td>1.262663</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=2000, training_loss=1.7592032198905945, metrics={'train_runtime': 733.814, 'train_samples_per_second': 86.878, 'train_steps_per_second': 2.725, 'total_flos': 4956004181606400.0, 'train_loss': 1.7592032198905945, 'epoch': 8.0})"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TjA9aBL4kUqV"
   },
   "source": [
    "Load pre-trained model and try to generate mini-story in another language."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lE_QVLCOkUqV",
    "outputId": "cf3ddb1a-5e89-473d-eeb8-7e32d87b7cb1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device type: cuda\n"
     ]
    }
   ],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "print(f\"Device type: {device}\")\n",
    "\n",
    "new_model_path = \"llama-42m-zh-fairytales/checkpoint-2000\" # saved checkpoint path\n",
    "model = LlamaForCausalLM.from_pretrained(new_model_path).to(device)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path, device=device)\n",
    "\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hlX2XilNkUqV"
   },
   "source": [
    "Evaluate the PPL on Chinese text (or another language) again.\n",
    "\n",
    "You will notice that we actually achieve a much lower PPL after continual pre-training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 278
    },
    "id": "0Ob_g328cGq5",
    "outputId": "e7f2b619-f3c8-4dc2-bbc2-76fac657b2b6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Perplexity: 3.53\n",
      "Test Perplexity: 3.52\n"
     ]
    }
   ],
   "source": [
    "data_file = TEST_FILE\n",
    "test_dataset = load_dataset('json', data_files={'test': data_file})[\"test\"][\"text\"]\n",
    "\n",
    "results = compute_ppl(model=model, tokenizer=tokenizer, device=device, inputs=test_dataset, batch_size = 16)\n",
    "dataset_ppl = results['mean_perplexity']\n",
    "print(f\"Test Perplexity: {dataset_ppl:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Xgi1EMDycZUp"
   },
   "source": [
    "---\n",
    "\n",
    "The original English base model was pre-trained on 2 million data samples. Considering we are using only 10,000 training samples (0.5% of the original pre-training data), the model can generate a few fluent sentences but may still struggle with long-text generation or common sense of other languages. You can try using more data or training steps depending on your computational resources.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Rf9ALF4CEX82",
    "outputId": "66d35223-1871-4d92-a008-cd50f257db4c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<s> 从前，有一只叫做汤姆的猫。汤姆和他的朋友们一起玩。他们喜欢在公园里玩耍。有一天，汤姆和他的朋友们决定去公园玩。\n",
      "\n",
      "在公园里，汤姆看到了一个大滑梯。他想玩滑梯。他跑去滑梯，但不小心掉在了地上。汤姆很伤心。\n",
      "\n",
      "汤姆很高兴他的朋友们能帮助他。他们一起玩滑梯，度过了很多快乐时光。\n"
     ]
    }
   ],
   "source": [
    "prompt = \"从前，有一只叫做汤姆的猫。汤姆和他的朋友们一起玩。\"\n",
    "\n",
    "# Decoding hyperparameters\n",
    "max_new_tokens = 300\n",
    "do_sample = True\n",
    "temperature = 0.3\n",
    "\n",
    "tokenized_input = tokenizer.encode(prompt, return_tensors=\"pt\").to(device)\n",
    "output_ids = model.generate(\n",
    "    tokenized_input,\n",
    "    max_new_tokens=max_new_tokens,\n",
    "    eos_token_id=1,\n",
    "    do_sample=do_sample,\n",
    "    temperature=temperature,\n",
    ")\n",
    "output_text = tokenizer.decode(output_ids[0])\n",
    "print(output_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JJYZCXX3kUqZ"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "037842bf542143038269aedb14c51c0d": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "19620102f85d4c6a8b48e1078a180e95": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "1a50911f63024bf59da9873dd6d1a912": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_e9b6ee204f4a457ab9863c0f3baf4864",
       "IPY_MODEL_40a0c17e7fde4bf2862ecda678d5e963",
       "IPY_MODEL_e0cdab58ab0c4936878db9d524c8726c"
      ],
      "layout": "IPY_MODEL_b2143d465f7047629cdfe9777e3a286b"
     }
    },
    "2ae1f05679b84ac9bac39c77485e6b01": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "2c286ad6343a475d8516ac4779b208ac": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_fa04910012354222b3f1a49ed5e33203",
      "max": 1,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_b45c1921cebe42418c9bac889be111de",
      "value": 1
     }
    },
    "2df92583f44640f38112db08ea30cc5e": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_9ef56642df394267850e820ae578d3d7",
       "IPY_MODEL_fb11f7b6a113420cbf9353c79f434ad5",
       "IPY_MODEL_b039f8470ffa4535ad065fc0eda886fd"
      ],
      "layout": "IPY_MODEL_d2f6deec117a4358ad064db89f727bc1"
     }
    },
    "325cd0c8a932466d9c115ffba2904bba": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "35d813e7b32f44e49d38155e4f950c86": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_4245c28c8eec4edd921b1755ad1ad45b",
       "IPY_MODEL_2c286ad6343a475d8516ac4779b208ac",
       "IPY_MODEL_62beee732f344d1fabf7e213930ad775"
      ],
      "layout": "IPY_MODEL_037842bf542143038269aedb14c51c0d"
     }
    },
    "35fe5e0e1ce14594b643f9679ea2b7f3": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "40a0c17e7fde4bf2862ecda678d5e963": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_76f64cb2e08e453d977b20150745fe41",
      "max": 63,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_35fe5e0e1ce14594b643f9679ea2b7f3",
      "value": 63
     }
    },
    "4245c28c8eec4edd921b1755ad1ad45b": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_7f004def454c46f4a489d83a265577f6",
      "placeholder": "​",
      "style": "IPY_MODEL_5407caa8f2df48179033c1d0d6daebfe",
      "value": "100%"
     }
    },
    "459a84f78164433884ba5f470a9e1417": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "5407caa8f2df48179033c1d0d6daebfe": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "62beee732f344d1fabf7e213930ad775": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_645cdc1efc3344dc8625f9ed98ee4433",
      "placeholder": "​",
      "style": "IPY_MODEL_b2f56d482fdd43dea48c9f649680e11b",
      "value": " 1/1 [00:00&lt;00:00, 19.33it/s]"
     }
    },
    "645cdc1efc3344dc8625f9ed98ee4433": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "67c58390dc914bb6a8db182ab1943157": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "76f64cb2e08e453d977b20150745fe41": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "7f004def454c46f4a489d83a265577f6": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "8995660b5da643a495cc5d8d23104913": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "91f68453eca74af8804b1871a20a0b26": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "9ef56642df394267850e820ae578d3d7": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_459a84f78164433884ba5f470a9e1417",
      "placeholder": "​",
      "style": "IPY_MODEL_19620102f85d4c6a8b48e1078a180e95",
      "value": "100%"
     }
    },
    "b039f8470ffa4535ad065fc0eda886fd": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_e063f6ac5f4a44b88271e700e985185a",
      "placeholder": "​",
      "style": "IPY_MODEL_325cd0c8a932466d9c115ffba2904bba",
      "value": " 63/63 [00:48&lt;00:00,  1.54it/s]"
     }
    },
    "b2143d465f7047629cdfe9777e3a286b": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "b2f56d482fdd43dea48c9f649680e11b": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "b45c1921cebe42418c9bac889be111de": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "b5769bc0ec3e4662b08fc27d8dda40c8": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "d2f6deec117a4358ad064db89f727bc1": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "e063f6ac5f4a44b88271e700e985185a": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "e0cdab58ab0c4936878db9d524c8726c": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_b5769bc0ec3e4662b08fc27d8dda40c8",
      "placeholder": "​",
      "style": "IPY_MODEL_91f68453eca74af8804b1871a20a0b26",
      "value": " 63/63 [01:18&lt;00:00,  1.07s/it]"
     }
    },
    "e56cd9f3c4124896a1f04d55b19888ea": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "e9b6ee204f4a457ab9863c0f3baf4864": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_e56cd9f3c4124896a1f04d55b19888ea",
      "placeholder": "​",
      "style": "IPY_MODEL_67c58390dc914bb6a8db182ab1943157",
      "value": "100%"
     }
    },
    "fa04910012354222b3f1a49ed5e33203": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "fb11f7b6a113420cbf9353c79f434ad5": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_8995660b5da643a495cc5d8d23104913",
      "max": 63,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_2ae1f05679b84ac9bac39c77485e6b01",
      "value": 63
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
